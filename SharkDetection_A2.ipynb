{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SharkDetection - A2",
      "provenance": [],
      "collapsed_sections": [
        "iIlFpG0IQjGS",
        "V5AVIEvWQWsG",
        "Uctfn5t6QPSG",
        "dRlY2hDSQKxH"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ_aKWdRp6Mv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Shark and Surfers Spottting with ImageAI\n",
        "Teja Kalvakolanu, Scott Rizzo, Luis Trujillo, Roxanne Miller, Nicky Kyono, Allison Erin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppZwMBJquRC",
        "colab_type": "text"
      },
      "source": [
        "# Goals\n",
        "\n",
        "\n",
        "*   To implement a program that can detect sharks and surfers in static images\n",
        "*   Get familair with ImageAI\n",
        "*   Using ImageAI with YOLOv3 pretrained model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zoCEYx3pTSG",
        "colab_type": "text"
      },
      "source": [
        "# OverView\n",
        "\n",
        "This lab will explain the process of detecting sharks in static images when working with ImageAI and YOLOv3. We will then use the trained model and apply it to object detection in videos.\n",
        "\n",
        "In this Colab Notebook we will show you how to:\n",
        "\n",
        "*   Install needed packages\n",
        "*   Import google drive and mount drive to colab\n",
        "*   Train Models using collected data with transfer learning\n",
        "*   Evaluate trained models and pick best one\n",
        "*   Detect Shark in static images\n",
        "*   Detect Sharks in videos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOyE6VGut-Ln",
        "colab_type": "text"
      },
      "source": [
        "# Colab Notebooks\n",
        "\n",
        "Colaboratory is a Google research project created to help disseminate machine learning education and research. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.\n",
        "\n",
        "Colaboratory allows you to create individual cells for writing documentation or python code. It allows you to run each cell separately or all at once. \n",
        "\n",
        "Google Colab provides a single 25GB NVIDIA Tesla K80 GPU that can be used up to 12 hours continuously. Recently, Colab also started offering free TPU. This made google colab the perfect options since to won such technology that google has made available would be quite expensive. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1zgmc121lti",
        "colab_type": "text"
      },
      "source": [
        "# Working with Colab\n",
        "\n",
        "To use Colab, cells containing code need to be run on the virtual environment. To run an individual cell click on the play button located at the top left of each cell. \n",
        "\n",
        "If a new model is not going to be trained then it is easier to load in our pre-trained shark detection model. In this case, skip running the training cell.\n",
        "\n",
        "If a new model is going to be trained, run the entire Colab Notebook. To run the entre notebook go to Runtime->Run all. .Keep in mind that the training step can take 12+ hours.\n",
        "\n",
        "If you need more information or want a quick crash course in Colab you can do this [Getting Started With Google Colab Guide](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrfyYE11wVoC",
        "colab_type": "text"
      },
      "source": [
        "# ImageAI\n",
        "ImageAI is the main tool we chose to go with which is an open-source python library that utilizes Deep Learning and Computer Vision. With imageAI we are able to train our own models, evaluate those models,and detect sharks in overhead images similar to a flying drones view. In addition we are using transfer learning for a pre-trained YOLO v3 to detect sharks and surfers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oACGImjdxjWL",
        "colab_type": "text"
      },
      "source": [
        "# Background\n",
        "Our project aims to identify sharks in the water using AI. The AI should be able to take in footage and recognize sharks and in the water, and eventually surfers and seals. We are using deep learning tools like image AI and tensorflow  which will take in images and build a model that can identify objects in images as well as videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxmTdMHao20D",
        "colab_type": "text"
      },
      "source": [
        "# Tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIlFpG0IQjGS",
        "colab_type": "text"
      },
      "source": [
        "### Install\n",
        "\n",
        "Along with installing ImageAI install the necessary dependencies that ImageAI runs on.\n",
        "- We need tensforflow version 1.13.1 for ImageAi as it doesn't currently support later versions as of yet\n",
        "- Due to the above requirement we also need to install the tensforflow-gpu version 1.13.1\n",
        "- Finally install imageai so can utilize the library to train, evaluate, and finally detect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQeMq2rdUKFV",
        "colab_type": "code",
        "outputId": "d78c599a-2415-47f1-fe85-398fe5494c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install tensorflow=1.13.1\n",
        "!pip3 install tensorflow-gpu==1.13.1\n",
        "!pip3 install imageai --upgrade"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'shark-spotting'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 34 (delta 8), reused 29 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 52.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (42.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n",
            "Collecting imageai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/99/4023e191a343fb23f01ae02ac57a5ca58037c310e8d8c62f87638a3bafc7/imageai-2.1.5-py3-none-any.whl (180kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from imageai) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from imageai) (3.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from imageai) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from imageai) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from imageai) (4.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (2.4.5)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->imageai) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageai) (0.46)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imageai) (42.0.1)\n",
            "Installing collected packages: imageai\n",
            "Successfully installed imageai-2.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5AVIEvWQWsG",
        "colab_type": "text"
      },
      "source": [
        "### Import Google Drive and mount\n",
        "\n",
        "Mount the Google Drive directory containing the shark dataset. \n",
        "- Mounting our google drive will allow us to use it as our own cloud environment\n",
        "- Our google drive contains all the images for training our models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAbBd8s5xYLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uctfn5t6QPSG",
        "colab_type": "text"
      },
      "source": [
        "### Training of Models\n",
        "\n",
        "This step is used to train a new model using transfer learning. Skip this step if you wish to just use our pretrained model.\n",
        "\n",
        "* Change **data_directory=\"*`{YOUR DIRECTORY HERE}`*\"** \n",
        "to provide your own dataset. An important requirement is that the videos show sharks at an overhead perspective. This is due to the given perspective that a drone would have flying over the water and was the way that the model was trained.\n",
        "\n",
        "* Change **train_from_pretrained_model=\"*`{YOUR DIRECTORY HERE}`*\"** to provide your own pretrained model.\n",
        "\n",
        "* You can also modify the **batch_size=`{SIZE}`** and **num_experiments=`{SIZE}`** to make sure the run time will complete its training before the Colab 12 hour limit is reached. Notice how this run time was only able to reach Epoch 43/100.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld1qSivNyaKf",
        "colab_type": "code",
        "outputId": "745ef36a-48fe-4a54-dec1-c8e82533ab51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from imageai.Detection.Custom import DetectionModelTrainer\n",
        "\n",
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/SharkDetection\")\n",
        "trainer.setTrainConfig(object_names_array=[\"shark\", \"surfer\", \"person\", \"boat\"], batch_size=8, num_experiments=100, train_from_pretrained_model=\"/content/gdrive/My Drive/480/pretrained-yolov3.h5\")\n",
        "trainer.trainModel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating anchor boxes for training images and annotation...\n",
            "Average IOU for 9 anchors: 0.75\n",
            "Anchor Boxes generated.\n",
            "Detection configuration saved in  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/json/detection_config.json\n",
            "Training on: \t['boat', 'person', 'shark', 'surfer']\n",
            "Training with Batch Size:  8\n",
            "Number of Experiments:  100\n",
            "Training with transfer learning from pretrained Model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "  warnings.warn('`epsilon` argument is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "592/592 [==============================] - 1003s 2s/step - loss: 45.6020 - yolo_layer_13_loss: 5.1902 - yolo_layer_14_loss: 13.3870 - yolo_layer_15_loss: 27.0248 - val_loss: 20.5505 - val_yolo_layer_13_loss: 2.5490 - val_yolo_layer_14_loss: 6.5878 - val_yolo_layer_15_loss: 11.4138\n",
            "Epoch 2/100\n",
            "592/592 [==============================] - 949s 2s/step - loss: 19.2325 - yolo_layer_13_loss: 2.6437 - yolo_layer_14_loss: 5.8480 - yolo_layer_15_loss: 10.7408 - val_loss: 17.1831 - val_yolo_layer_13_loss: 2.6417 - val_yolo_layer_14_loss: 5.0370 - val_yolo_layer_15_loss: 9.5044\n",
            "Epoch 3/100\n",
            "592/592 [==============================] - 939s 2s/step - loss: 15.4009 - yolo_layer_13_loss: 2.0055 - yolo_layer_14_loss: 4.0182 - yolo_layer_15_loss: 9.3771 - val_loss: 15.2215 - val_yolo_layer_13_loss: 2.2517 - val_yolo_layer_14_loss: 4.3917 - val_yolo_layer_15_loss: 8.5781\n",
            "Epoch 4/100\n",
            "592/592 [==============================] - 948s 2s/step - loss: 14.5940 - yolo_layer_13_loss: 2.0175 - yolo_layer_14_loss: 3.8526 - yolo_layer_15_loss: 8.7239 - val_loss: 14.4505 - val_yolo_layer_13_loss: 2.2424 - val_yolo_layer_14_loss: 4.3588 - val_yolo_layer_15_loss: 7.8493\n",
            "Epoch 5/100\n",
            "592/592 [==============================] - 962s 2s/step - loss: 13.4762 - yolo_layer_13_loss: 1.8509 - yolo_layer_14_loss: 3.6912 - yolo_layer_15_loss: 7.9340 - val_loss: 13.8504 - val_yolo_layer_13_loss: 2.0224 - val_yolo_layer_14_loss: 4.0875 - val_yolo_layer_15_loss: 7.7405\n",
            "Epoch 6/100\n",
            "592/592 [==============================] - 916s 2s/step - loss: 12.5439 - yolo_layer_13_loss: 1.6232 - yolo_layer_14_loss: 3.4309 - yolo_layer_15_loss: 7.4898 - val_loss: 13.2515 - val_yolo_layer_13_loss: 1.9128 - val_yolo_layer_14_loss: 3.9214 - val_yolo_layer_15_loss: 7.4173\n",
            "Epoch 7/100\n",
            "592/592 [==============================] - 958s 2s/step - loss: 12.0176 - yolo_layer_13_loss: 1.6841 - yolo_layer_14_loss: 3.4309 - yolo_layer_15_loss: 6.9026 - val_loss: 13.2936 - val_yolo_layer_13_loss: 2.2153 - val_yolo_layer_14_loss: 3.7190 - val_yolo_layer_15_loss: 7.3593\n",
            "Epoch 8/100\n",
            "592/592 [==============================] - 920s 2s/step - loss: 11.6474 - yolo_layer_13_loss: 1.5439 - yolo_layer_14_loss: 3.2980 - yolo_layer_15_loss: 6.8056 - val_loss: 12.8019 - val_yolo_layer_13_loss: 1.3747 - val_yolo_layer_14_loss: 4.3120 - val_yolo_layer_15_loss: 7.1152\n",
            "Epoch 9/100\n",
            "592/592 [==============================] - 955s 2s/step - loss: 11.5484 - yolo_layer_13_loss: 1.5635 - yolo_layer_14_loss: 3.2737 - yolo_layer_15_loss: 6.7112 - val_loss: 12.8372 - val_yolo_layer_13_loss: 1.9236 - val_yolo_layer_14_loss: 3.6522 - val_yolo_layer_15_loss: 7.2614\n",
            "Epoch 10/100\n",
            "592/592 [==============================] - 927s 2s/step - loss: 11.2278 - yolo_layer_13_loss: 1.4909 - yolo_layer_14_loss: 3.1265 - yolo_layer_15_loss: 6.6103 - val_loss: 13.9821 - val_yolo_layer_13_loss: 2.2599 - val_yolo_layer_14_loss: 4.3584 - val_yolo_layer_15_loss: 7.3639\n",
            "Epoch 11/100\n",
            "592/592 [==============================] - 954s 2s/step - loss: 11.0799 - yolo_layer_13_loss: 1.4620 - yolo_layer_14_loss: 3.0591 - yolo_layer_15_loss: 6.5588 - val_loss: 13.1234 - val_yolo_layer_13_loss: 2.3974 - val_yolo_layer_14_loss: 3.9082 - val_yolo_layer_15_loss: 6.8178\n",
            "Epoch 12/100\n",
            "592/592 [==============================] - 958s 2s/step - loss: 10.8212 - yolo_layer_13_loss: 1.4373 - yolo_layer_14_loss: 3.1012 - yolo_layer_15_loss: 6.2828 - val_loss: 13.4208 - val_yolo_layer_13_loss: 2.6782 - val_yolo_layer_14_loss: 4.0766 - val_yolo_layer_15_loss: 6.6660\n",
            "Epoch 13/100\n",
            "592/592 [==============================] - 990s 2s/step - loss: 10.7330 - yolo_layer_13_loss: 1.5250 - yolo_layer_14_loss: 3.1910 - yolo_layer_15_loss: 6.0170 - val_loss: 12.2236 - val_yolo_layer_13_loss: 1.6159 - val_yolo_layer_14_loss: 3.7114 - val_yolo_layer_15_loss: 6.8963\n",
            "Epoch 14/100\n",
            "592/592 [==============================] - 932s 2s/step - loss: 10.5694 - yolo_layer_13_loss: 1.3502 - yolo_layer_14_loss: 2.9125 - yolo_layer_15_loss: 6.3068 - val_loss: 12.7284 - val_yolo_layer_13_loss: 1.8990 - val_yolo_layer_14_loss: 4.1552 - val_yolo_layer_15_loss: 6.6742\n",
            "Epoch 15/100\n",
            "592/592 [==============================] - 986s 2s/step - loss: 10.3358 - yolo_layer_13_loss: 1.3780 - yolo_layer_14_loss: 2.9899 - yolo_layer_15_loss: 5.9680 - val_loss: 12.8582 - val_yolo_layer_13_loss: 1.4184 - val_yolo_layer_14_loss: 4.3458 - val_yolo_layer_15_loss: 7.0940\n",
            "Epoch 16/100\n",
            "592/592 [==============================] - 950s 2s/step - loss: 10.2215 - yolo_layer_13_loss: 1.3164 - yolo_layer_14_loss: 2.8916 - yolo_layer_15_loss: 6.0135 - val_loss: 13.1249 - val_yolo_layer_13_loss: 1.7573 - val_yolo_layer_14_loss: 4.3186 - val_yolo_layer_15_loss: 7.0489\n",
            "Epoch 17/100\n",
            "592/592 [==============================] - 940s 2s/step - loss: 10.2519 - yolo_layer_13_loss: 1.3200 - yolo_layer_14_loss: 2.9824 - yolo_layer_15_loss: 5.9495 - val_loss: 13.6182 - val_yolo_layer_13_loss: 2.1997 - val_yolo_layer_14_loss: 4.5566 - val_yolo_layer_15_loss: 6.8619\n",
            "Epoch 18/100\n",
            "592/592 [==============================] - 943s 2s/step - loss: 9.9038 - yolo_layer_13_loss: 1.2619 - yolo_layer_14_loss: 2.8840 - yolo_layer_15_loss: 5.7579 - val_loss: 13.1636 - val_yolo_layer_13_loss: 1.8376 - val_yolo_layer_14_loss: 4.1847 - val_yolo_layer_15_loss: 7.1414\n",
            "Epoch 19/100\n",
            "592/592 [==============================] - 952s 2s/step - loss: 9.7798 - yolo_layer_13_loss: 1.2104 - yolo_layer_14_loss: 2.8220 - yolo_layer_15_loss: 5.7474 - val_loss: 12.9207 - val_yolo_layer_13_loss: 1.9989 - val_yolo_layer_14_loss: 4.3134 - val_yolo_layer_15_loss: 6.6084\n",
            "Epoch 20/100\n",
            "592/592 [==============================] - 948s 2s/step - loss: 9.8219 - yolo_layer_13_loss: 1.3123 - yolo_layer_14_loss: 2.9095 - yolo_layer_15_loss: 5.6000 - val_loss: 12.1767 - val_yolo_layer_13_loss: 1.5090 - val_yolo_layer_14_loss: 3.8181 - val_yolo_layer_15_loss: 6.8497\n",
            "Epoch 21/100\n",
            "592/592 [==============================] - 943s 2s/step - loss: 9.5542 - yolo_layer_13_loss: 1.1726 - yolo_layer_14_loss: 2.7559 - yolo_layer_15_loss: 5.6257 - val_loss: 13.2446 - val_yolo_layer_13_loss: 2.4100 - val_yolo_layer_14_loss: 4.0310 - val_yolo_layer_15_loss: 6.8036\n",
            "Epoch 22/100\n",
            "592/592 [==============================] - 967s 2s/step - loss: 9.6148 - yolo_layer_13_loss: 1.2833 - yolo_layer_14_loss: 2.8115 - yolo_layer_15_loss: 5.5200 - val_loss: 13.1645 - val_yolo_layer_13_loss: 2.1448 - val_yolo_layer_14_loss: 4.2355 - val_yolo_layer_15_loss: 6.7841\n",
            "Epoch 23/100\n",
            "592/592 [==============================] - 937s 2s/step - loss: 9.4732 - yolo_layer_13_loss: 1.1668 - yolo_layer_14_loss: 2.8020 - yolo_layer_15_loss: 5.5044 - val_loss: 13.1679 - val_yolo_layer_13_loss: 1.8065 - val_yolo_layer_14_loss: 4.7085 - val_yolo_layer_15_loss: 6.6529\n",
            "Epoch 24/100\n",
            "592/592 [==============================] - 989s 2s/step - loss: 9.6198 - yolo_layer_13_loss: 1.3003 - yolo_layer_14_loss: 2.8568 - yolo_layer_15_loss: 5.4627 - val_loss: 12.9565 - val_yolo_layer_13_loss: 1.6971 - val_yolo_layer_14_loss: 4.4237 - val_yolo_layer_15_loss: 6.8357\n",
            "Epoch 25/100\n",
            "592/592 [==============================] - 962s 2s/step - loss: 9.3079 - yolo_layer_13_loss: 1.1415 - yolo_layer_14_loss: 2.6781 - yolo_layer_15_loss: 5.4882 - val_loss: 12.8875 - val_yolo_layer_13_loss: 1.9104 - val_yolo_layer_14_loss: 3.9048 - val_yolo_layer_15_loss: 7.0723\n",
            "Epoch 26/100\n",
            "592/592 [==============================] - 904s 2s/step - loss: 9.3353 - yolo_layer_13_loss: 1.0885 - yolo_layer_14_loss: 2.6538 - yolo_layer_15_loss: 5.5930 - val_loss: 12.8267 - val_yolo_layer_13_loss: 2.0956 - val_yolo_layer_14_loss: 4.2293 - val_yolo_layer_15_loss: 6.5018\n",
            "Epoch 27/100\n",
            "592/592 [==============================] - 951s 2s/step - loss: 9.0726 - yolo_layer_13_loss: 1.1426 - yolo_layer_14_loss: 2.6582 - yolo_layer_15_loss: 5.2718 - val_loss: 13.5002 - val_yolo_layer_13_loss: 2.1293 - val_yolo_layer_14_loss: 4.7163 - val_yolo_layer_15_loss: 6.6546\n",
            "Epoch 28/100\n",
            "592/592 [==============================] - 918s 2s/step - loss: 9.0733 - yolo_layer_13_loss: 1.1180 - yolo_layer_14_loss: 2.5435 - yolo_layer_15_loss: 5.4118 - val_loss: 12.7204 - val_yolo_layer_13_loss: 1.8531 - val_yolo_layer_14_loss: 4.2233 - val_yolo_layer_15_loss: 6.6440\n",
            "Epoch 29/100\n",
            "592/592 [==============================] - 971s 2s/step - loss: 9.1149 - yolo_layer_13_loss: 1.1415 - yolo_layer_14_loss: 2.6932 - yolo_layer_15_loss: 5.2802 - val_loss: 12.2121 - val_yolo_layer_13_loss: 1.6070 - val_yolo_layer_14_loss: 3.9781 - val_yolo_layer_15_loss: 6.6271\n",
            "Epoch 30/100\n",
            "592/592 [==============================] - 911s 2s/step - loss: 8.3269 - yolo_layer_13_loss: 0.9536 - yolo_layer_14_loss: 2.4115 - yolo_layer_15_loss: 4.9618 - val_loss: 12.3729 - val_yolo_layer_13_loss: 1.5537 - val_yolo_layer_14_loss: 4.3798 - val_yolo_layer_15_loss: 6.4393\n",
            "Epoch 31/100\n",
            "592/592 [==============================] - 911s 2s/step - loss: 8.0534 - yolo_layer_13_loss: 0.8617 - yolo_layer_14_loss: 2.3162 - yolo_layer_15_loss: 4.8755 - val_loss: 12.6657 - val_yolo_layer_13_loss: 1.9187 - val_yolo_layer_14_loss: 4.5512 - val_yolo_layer_15_loss: 6.1958\n",
            "Epoch 32/100\n",
            "592/592 [==============================] - 925s 2s/step - loss: 7.7685 - yolo_layer_13_loss: 0.8337 - yolo_layer_14_loss: 2.2640 - yolo_layer_15_loss: 4.6708 - val_loss: 12.7557 - val_yolo_layer_13_loss: 1.7977 - val_yolo_layer_14_loss: 4.2742 - val_yolo_layer_15_loss: 6.6838\n",
            "Epoch 33/100\n",
            "592/592 [==============================] - 954s 2s/step - loss: 7.9405 - yolo_layer_13_loss: 0.8675 - yolo_layer_14_loss: 2.3079 - yolo_layer_15_loss: 4.7651 - val_loss: 13.0035 - val_yolo_layer_13_loss: 1.8341 - val_yolo_layer_14_loss: 4.6865 - val_yolo_layer_15_loss: 6.4829\n",
            "Epoch 34/100\n",
            "592/592 [==============================] - 950s 2s/step - loss: 7.8786 - yolo_layer_13_loss: 0.9768 - yolo_layer_14_loss: 2.3389 - yolo_layer_15_loss: 4.5629 - val_loss: 12.4060 - val_yolo_layer_13_loss: 1.6095 - val_yolo_layer_14_loss: 4.2412 - val_yolo_layer_15_loss: 6.5553\n",
            "Epoch 35/100\n",
            "592/592 [==============================] - 928s 2s/step - loss: 7.6620 - yolo_layer_13_loss: 0.8446 - yolo_layer_14_loss: 2.2087 - yolo_layer_15_loss: 4.6087 - val_loss: 12.3657 - val_yolo_layer_13_loss: 1.7652 - val_yolo_layer_14_loss: 4.2184 - val_yolo_layer_15_loss: 6.3821\n",
            "Epoch 36/100\n",
            "592/592 [==============================] - 974s 2s/step - loss: 7.8184 - yolo_layer_13_loss: 0.9963 - yolo_layer_14_loss: 2.3216 - yolo_layer_15_loss: 4.5006 - val_loss: 12.2753 - val_yolo_layer_13_loss: 1.4208 - val_yolo_layer_14_loss: 4.3584 - val_yolo_layer_15_loss: 6.4961\n",
            "Epoch 37/100\n",
            "592/592 [==============================] - 950s 2s/step - loss: 7.9788 - yolo_layer_13_loss: 0.9029 - yolo_layer_14_loss: 2.3737 - yolo_layer_15_loss: 4.7022 - val_loss: 12.5848 - val_yolo_layer_13_loss: 1.6911 - val_yolo_layer_14_loss: 4.5018 - val_yolo_layer_15_loss: 6.3919\n",
            "Epoch 38/100\n",
            "592/592 [==============================] - 974s 2s/step - loss: 7.7755 - yolo_layer_13_loss: 0.9440 - yolo_layer_14_loss: 2.3353 - yolo_layer_15_loss: 4.4962 - val_loss: 12.3461 - val_yolo_layer_13_loss: 1.6062 - val_yolo_layer_14_loss: 4.2932 - val_yolo_layer_15_loss: 6.4466\n",
            "Epoch 39/100\n",
            "592/592 [==============================] - 958s 2s/step - loss: 7.7701 - yolo_layer_13_loss: 0.8186 - yolo_layer_14_loss: 2.2702 - yolo_layer_15_loss: 4.6814 - val_loss: 12.4990 - val_yolo_layer_13_loss: 1.8216 - val_yolo_layer_14_loss: 4.4188 - val_yolo_layer_15_loss: 6.2585\n",
            "Epoch 40/100\n",
            "592/592 [==============================] - 971s 2s/step - loss: 7.7100 - yolo_layer_13_loss: 0.8664 - yolo_layer_14_loss: 2.2382 - yolo_layer_15_loss: 4.6054 - val_loss: 12.2738 - val_yolo_layer_13_loss: 1.8484 - val_yolo_layer_14_loss: 4.0593 - val_yolo_layer_15_loss: 6.3662\n",
            "Epoch 41/100\n",
            "592/592 [==============================] - 987s 2s/step - loss: 7.7490 - yolo_layer_13_loss: 1.0161 - yolo_layer_14_loss: 2.2850 - yolo_layer_15_loss: 4.4479 - val_loss: 12.1851 - val_yolo_layer_13_loss: 1.5338 - val_yolo_layer_14_loss: 4.0299 - val_yolo_layer_15_loss: 6.6215\n",
            "Epoch 42/100\n",
            "592/592 [==============================] - 943s 2s/step - loss: 7.6074 - yolo_layer_13_loss: 0.8295 - yolo_layer_14_loss: 2.1637 - yolo_layer_15_loss: 4.6142 - val_loss: 12.4434 - val_yolo_layer_13_loss: 1.7683 - val_yolo_layer_14_loss: 4.4129 - val_yolo_layer_15_loss: 6.2622\n",
            "Epoch 43/100\n",
            "119/592 [=====>........................] - ETA: 11:50 - loss: 7.8299 - yolo_layer_13_loss: 0.9560 - yolo_layer_14_loss: 2.4131 - yolo_layer_15_loss: 4.4609"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRlY2hDSQKxH",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of Models\n",
        "\n",
        "In this step we evaluate our model. This step is used for testing purposes. We set our thresholds so that we dont get bogged down from low confidence numbers. We evaluate the confidence in our object detection from the dataset we proveded and choose the best one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzlFOA2_zRkI",
        "colab_type": "code",
        "outputId": "18488f31-08ab-40cf-8608-b8c8abaacb47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from imageai.Detection.Custom import DetectionModelTrainer\n",
        "\n",
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection\")\n",
        "trainer.evaluateModel(model_path=\"/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models\", json_path=\"/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/json/detection_config.json\", iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/imageai/Detection/Custom/yolo.py:24: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-005--loss-0013.476.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0052\n",
            "shark: 0.9222\n",
            "surfer: 0.9059\n",
            "mAP: 0.4583\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-012--loss-0010.821.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0538\n",
            "shark: 0.9324\n",
            "surfer: 0.8769\n",
            "mAP: 0.4658\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-013--loss-0010.733.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0570\n",
            "shark: 0.9377\n",
            "surfer: 0.9088\n",
            "mAP: 0.4759\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-014--loss-0010.569.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0715\n",
            "shark: 0.9341\n",
            "surfer: 0.9370\n",
            "mAP: 0.4857\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-015--loss-0010.336.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0246\n",
            "shark: 0.8933\n",
            "surfer: 0.8440\n",
            "mAP: 0.4405\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-016--loss-0010.222.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0389\n",
            "shark: 0.8896\n",
            "surfer: 0.8476\n",
            "mAP: 0.4440\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-018--loss-0009.904.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.1091\n",
            "shark: 0.8954\n",
            "surfer: 0.9138\n",
            "mAP: 0.4796\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-019--loss-0009.780.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0278\n",
            "shark: 0.8957\n",
            "surfer: 0.9055\n",
            "mAP: 0.4573\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-021--loss-0009.554.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.0961\n",
            "shark: 0.9247\n",
            "surfer: 0.8878\n",
            "mAP: 0.4772\n",
            "===============================\n",
            "Model File:  /content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-023--loss-0009.473.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "boat: 0.0000\n",
            "person: 0.1250\n",
            "shark: 0.8991\n",
            "surfer: 0.9140\n",
            "mAP: 0.4845\n",
            "===============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'average_precision': {'boat': 0,\n",
              "   'person': 0.005208333333333333,\n",
              "   'shark': 0.9222443886274947,\n",
              "   'surfer': 0.9058899101998319},\n",
              "  'map': 0.45833565804016496,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-005--loss-0013.476.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.05381944444444445,\n",
              "   'shark': 0.9323547372260454,\n",
              "   'surfer': 0.8768842963980601},\n",
              "  'map': 0.4657646195171375,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-012--loss-0010.821.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.05701754385964912,\n",
              "   'shark': 0.9376676605423945,\n",
              "   'surfer': 0.9088058060717867},\n",
              "  'map': 0.4758727526184576,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-013--loss-0010.733.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.07146646859083192,\n",
              "   'shark': 0.9341437116181224,\n",
              "   'surfer': 0.9370228718696705},\n",
              "  'map': 0.4856582630196562,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-014--loss-0010.569.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.024553571428571428,\n",
              "   'shark': 0.8933387580126311,\n",
              "   'surfer': 0.844005746629634},\n",
              "  'map': 0.4404745190177091,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-015--loss-0010.336.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.03894927536231884,\n",
              "   'shark': 0.8895534506926844,\n",
              "   'surfer': 0.8476408419636555},\n",
              "  'map': 0.4440358920046647,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-016--loss-0010.222.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.10905032467532466,\n",
              "   'shark': 0.8953654151822021,\n",
              "   'surfer': 0.9138496206476698},\n",
              "  'map': 0.4795663401262992,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-018--loss-0009.904.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.027773268398268398,\n",
              "   'shark': 0.8957104272283167,\n",
              "   'surfer': 0.9055211584394474},\n",
              "  'map': 0.4572512135165081,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-019--loss-0009.780.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.0961428752535497,\n",
              "   'shark': 0.9247142227959588,\n",
              "   'surfer': 0.8877520088248126},\n",
              "  'map': 0.4771522767185803,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-021--loss-0009.554.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3},\n",
              " {'average_precision': {'boat': 0,\n",
              "   'person': 0.12499999999999997,\n",
              "   'shark': 0.8990626742730776,\n",
              "   'surfer': 0.9139854637425927},\n",
              "  'map': 0.48451203450391755,\n",
              "  'model_file': '/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-023--loss-0009.473.h5',\n",
              "  'using_iou': 0.5,\n",
              "  'using_non_maximum_suppression': 0.5,\n",
              "  'using_object_threshold': 0.3}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BssluijypXBY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcg-DdyPyVGY",
        "colab_type": "text"
      },
      "source": [
        "# Shark detection in Videos (Results)\n",
        "\n",
        "Apply the shark detection model to a video with sharks. \n",
        "\n",
        "If you wish to change the video (the video *must* be at an overhead perspective of the sharks) we provided replace the path in **video_detector.setModelPath(\"`{YOUR DIRECTORY HERE}`\")** to the video you want to test. As well as what you want to call the filename for the video in **output_file_path=os.path.join(\"/*path*\", \"/*path*/`{YOUR FILE NAME HERE}`\")**\n",
        "\n",
        "The resulting video with sharks and surfers detected can be found at the path displayed below once the cell is done. \n",
        "\n",
        "The final confidence is over 90% for sharks and over 80% for surfers. This is better than we expected for shark identification. The results for surfers are also good but as in some cases the model identifies surfers as both sharks and surfers. This model does not classify non-sharks or non-surfers as \"other\" due to a lack of training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e4Hy0mPslC_",
        "colab_type": "code",
        "outputId": "c43ee387-1d50-43d9-b720-b8d919e8f7a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from imageai.Detection.Custom import CustomVideoObjectDetection\n",
        "import os\n",
        "\n",
        "execution_path = os.getcwd()\n",
        "\n",
        "video_detector = CustomVideoObjectDetection()\n",
        "video_detector.setModelTypeAsYOLOv3()\n",
        "video_detector.setModelPath(\"/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/models/detection_model-ex-014--loss-0010.569.h5\")\n",
        "video_detector.setJsonPath(\"/content/gdrive/My Drive/480/SharkDetectionV2/SharkDetection/json/detection_config.json\")\n",
        "video_detector.loadModel()\n",
        "\n",
        "video_detector.detectObjectsFromVideo(input_file_path=\"/content/gdrive/My Drive/480/SharkDetectionV2/leopardSharkTest1.mp4\",\n",
        "                                          output_file_path=os.path.join(\"/content/gdrive/My Drive/480/SharkDetectionV2/\", \"/content/gdrive/My Drive/480/SharkDetectionV2/LeopardResults1\"),\n",
        "                                          frames_per_second=30,\n",
        "                                          minimum_percentage_probability=30,\n",
        "                                          log_progress=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/480/SharkDetectionV2/LeopardResults1.avi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isrJsUVThZTg",
        "colab_type": "text"
      },
      "source": [
        "### Additional Resources\n",
        "Additional documentation on ImageAI can be found at https://imageai.readthedocs.io/en/latest/\n",
        "\n",
        "LabelImg labeling program we used - https://github.com/tzutalin/labelImg\n",
        "\n"
      ]
    }
  ]
}